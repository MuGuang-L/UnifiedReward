# **实验报告：Qwen2.5-VL-3B奖励模型微调的深度对比分析(全参 vs. LoRA)**

**关联任务 Issue**: [补充lora训练方案 · Issue #6 · Tencent-Hunyuan/UnifiedReward](https://github.com/Tencent-Hunyuan/UnifiedReward/issues/6)

> ### **核心摘要 **
>
> 本报告详细记录了在 Qwen2.5-VL-3B-Instruct 模型上，采用全参微调与LoRA两种技术方案进行奖励模型训练的全面对比。
>
> **核心结论**：**LoRA 微调在最终验证集Loss上仅比全参高出不到0.9%，几乎达到了与全参微调同等的性能水平。** 考虑到LoRA在训练时间、显存消耗上的巨大优势，**本报告明确推荐LoRA作为此场景下兼具性能与效率的最佳实践方案。**



## 1. 实验背景与目标

为响应社区“补充LoRA训练方案”的任务，本实验旨在为UnifiedReward项目提供一份关于Qwen2.5-VL-3B模型微调的、数据驱动的实践参考。核心目标包括：

- 在 ImageGen-CoT-Reward-5K 数据集上，对**全参微调与LoRA微调**进行严谨的并排实验。
- 从**最终性能**、**训练动态**及**工程效率**等多个维度，对两种方案进行深度的量化与可视化对比。
- 基于实验数据，提炼出清晰的结论与可行的实践建议。

## 2. 实验环境与配置

### 2.1 硬件与核心框架

- 硬件平台: NVIDIA RTX 4090 (48GB)
- **Pytorch版本**: 2.6.0+cu124
- **核心库**: transformers==4.50.0, peft==0.15.1

### 2.2 关键超参数对比

| 超参数 (Hyperparameter) | 全参微调               | LoRA 微调              | 备注与说明                                               |
| ----------------------- | ---------------------- | ---------------------- | -------------------------------------------------------- |
| base_model              | Qwen2.5-VL-3B-Instruct | Qwen2.5-VL-3B-Instruct | 共享同一基座模型                                         |
| **learning_rate**       | **2e-05**              | **1e-04**              | **关键差异点**：根据两种方法的特性选择了不同的最优学习率 |
| optimizer               | AdamW (8-bit)          | AdamW (8-bit)          | 采用节省显存的8位优化器                                  |
| lr_scheduler_type       | cosine                 | cosine                 |                                                          |
| num_epochs              | 3.0                    | 3.0                    |                                                          |
| effective_batch_size    | 8                      | 8                      | (单卡batch_size=1, 梯度累积=8)                           |
| **GPU显存占用(约)**     | [~45GB]                | [~15GB]                | LoRA的效率优势在此体现                                   |

## 3. 实验结果与深度分析

### 3.1 最终性能量化对比

| 微调方法     | 最终验证Loss (Final Validation Loss) |
| ------------ | ------------------------------------ |
| **全参微调** | **0.5438**                           |
| **LoRA微调** | **0.5484**                           |

> **分析**:
> 全参微调凭借其全量参数调整的优势，取得了最低的验证Loss。然而，LoRA的Loss值仅比SFT高出 **0.0046**（约 **0.85%**），性能差距微乎其微。这一数据强有力地证明，LoRA能够以极小的性能代价，换取巨大的效率提升。
>
> **注**：详细的训练与验证可视化信息见根目录下的tensorboard_logs文件夹。

### 3.2 训练过程可视化分析

为了深入理解全参和LoRA两种微调方法的训练动态，我们分别展示了它们的训练Loss和验证Loss曲线。

**图1: LoRA微调的训练Loss曲线**

![lora_train_loss](loss_figure/training_loss_lora.png)





**图2: LoRA微调的验证Loss曲线**

![lora_eval_loss](loss_figure/training_eval_loss_lora.png)



**图3: 全参微调的训练Loss曲线**



![full_train_loss](loss_figure/training_loss_full.png)



**图4: 全参微调的验证Loss曲线**



![full_eval_loss](loss_figure/training_eval_loss_full.png)

> **综合分析**:
>
> 1. **收敛趋势的共性**: 从图1和图3可以看出，全参和LoRA的训练Loss都呈现出健康的阶梯状下降趋势，表明模型在积极地学习训练数据。图2和图4则展示了验证Loss的持续平滑下降，这有力地证明了两种方法训练出的模型都具备良好的泛化能力。
>
> 2. **性能接近性**: 对比图2 (LoRA验证Loss) 和图4 (全参验证Loss)，尽管是分开展示，但它们在整体趋势和最终收敛值上表现出高度的一致性。这进一步印证了我们在3.1节中量化得出的结论：**LoRA微调在保持极低Loss差距的前提下，达到了与全参微调几乎相同的最终性能。**
>
> 3. **共同的“收益递减”区**: 无论是LoRA（图2）还是全参（图4），它们的验证Loss曲线在约 **1400-1600步** 后都变得极其平缓，呈现出明显的“平台期”。这一现象提示我们，在实际应用中，过度延长训练步数对模型性能的提升效益甚微，提前停止训练将是更高效的选择，尤其有助于充分发挥LoRA的成本优势。

### 3.3 收敛状态与工程实践考量

> **进一步思考**:
> 值得注意的是，从曲线末端的平缓下降趋势来看，模型理论上尚未达到数学意义上的“完全收敛”。**然而，考虑到极其显著的边际效益递减，我判断，为了追求那几乎可以忽略不计的Loss下降而投入大量额外的计算资源是不划算的。** 在真实的工程实践中，当前所达到的收敛状态已经完全足够，这种对“成本-效益”的权衡比追求理论上的极限值更有意义。


## 4. 量化性能评测与深度分析

在完成初步的Loss对比后，我们利用 `GenAI-Bench` 对不同检查点进行了量化性能评测。评测结果揭示了与训练损失趋势不一致的关键发现。

### 4.1 评测基准与核心指标

-   **评测工具**: `UnifiedReward` 项目提供的 `GenAI-Bench` 评估流程。
-   **评测任务**: 模型需判断由同一提示词生成的两张图片中的更优者。
-   **核心指标**: `Acc. w/o tie` (Accuracy without tie)，该指标排除了平局情况，能更精确地衡量模型在有明确偏好样本上的判断准确率。

### 4.2 评测结果对比

与训练/验证Loss的高度接近不同，`GenAI-Bench` 的评测准确率呈现出显著分化。

**表2: 全参 vs. LoRA 在不同检查点的性能对比 (Acc. w/o tie)**

| 微调方法     | 检查点 (Steps) | Acc. w/o tie | 相比基线提升 (Δ) |
| :----------- | :------------- | :----------- | :--------------- |
| 原始模型     | Baseline       | 39.03%       | -                |
| **全参微调** | **500**        | **59.76%**   | **+20.73%**      |
|              | 1000           | 54.51%       | +15.48%          |
|              | 1500           | 55.05%       | +16.02%          |
|              | 1971           | 56.53%       | +17.50%          |
| LoRA 微调    | 500            | 41.59%       | +2.56%           |
|              | 1000           | 41.18%       | +2.15%           |
|              | 1500           | 39.57%       | +0.54%           |
|              | 1971           | 41.05%       | +2.02%           |

### 4.3 可视化分析

为直观对比两种方案的有效性，将`Acc. w/o tie`指标随训练步数的变化进行可视化。

![evaluation on GenAI-Bench](evaluation\performance on GenAI-Bench.png)

**分析要点**:

1.  **性能鸿沟**: 全参微调带来了**超过20个百分点**的巨大性能飞跃，而LoRA的提升幅度始终在1-2%左右徘徊，未能实质性超越原始模型。
2.  **LoRA的能力瓶颈**: LoRA未能有效学习到完成此复杂评测任务所需的深层能力，其性能在训练全程中无明显增长，表明已触及该方法的能力上限。
3.  **全参微调的有效性**: 全参微调在500步时即达到性能峰值(59.76%)，证明了其在解锁模型固有视觉理解和逻辑判断能力上的高效性。后续步骤的性能回落可能与轻度过拟合有关。

### 4.4 核心发现：训练Loss与评测指标的“脱钩”

本实验的核心发现是：**模型在训练集上的拟合优度（由Loss衡量）与在下游复杂任务上的实际表现（由Benchmark衡量）可以完全脱钩。**

-   **拟合“格式” vs. 学习“能力”**: LoRA成功地降低了Loss，表明它学会了生成符合训练数据格式的答案文本。然而，它并未真正掌握判断图像优劣的**能力**。
-   **评测结果**: `GenAI-Bench` 直接评估判断结果的正确性，从而甄别出LoRA只是在“模仿答案”，而全参微调是真正学会了“如何判断”。

## 5. 修正结论与实践建议

基于量化评测结果，我们必须修正仅基于Loss得出的初步结论。

-   **最终结论**: **对于奖励模型微调这类需要复杂推理与判断能力的任务，全参微调是当前唯一被验证有效的技术路径。** LoRA在此场景下性能提升可忽略不计，其效率优势失去了意义。

-   **实践建议**:
    1.  **方法论**: 涉及高级认知能力的微调任务，应**首选全参微调**。
    2.  **检查点策略**: 全参微调的最佳性能出现在训练早期（`full_500`检查点）。因此，**建立基于下游任务的频繁验证和早期停止机制至关重要**。
    3.  **评估为王**: **绝对不能仅依赖训练/验证Loss来评估模型性能**。必须使用与最终应用目标一致的、独立的基准测试作为最终决策依据。

## 6. 下一步工作规划

为更深入地理解全参与LoRA在模型行为上的差异，下一步计划将聚焦于**定性案例分析**：

1.  **挑选典型案例**: 从`GenAI-Bench`中选取全参模型判断正确而LoRA判断错误的样本。
2.  **对比生成内容**: 对比`full_500`和`lora_500`模型生成的完整回答，从文本的逻辑、关注点和细节上，直观展示二者在图像理解能力上的本质差异。



## **待完善...**

------



在完成以上工作后，我会更新这份报告或提交新的分析。感谢您的阅读。
